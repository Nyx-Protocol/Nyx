// Multipath data transfer integration test
//
// Tests:
// - Concurrent data transfer over multiple network paths
// - Path failover behavior when one path degrades
// - Throughput measurement and comparison with single-path
// - PathBuilder integration with live network metrics
//
// Design principles:
// - Pure Rust implementation (NO C/C++ dependencies)
// - Realistic network simulation using TestNetwork
// - Automated quality verification
// - Comprehensive error handling

use crate::test_harness::{ClientHandle, DaemonConfig, NetworkConfig, TestHarness, TestResult};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use tracing::{info, warn};

/// Test payload size for throughput measurement (1 MB)
#[allow(dead_code)]
const TEST_PAYLOAD_SIZE: usize = 1024 * 1024;

/// Expected failover time threshold (5 seconds)
#[allow(dead_code)]
const FAILOVER_THRESHOLD: Duration = Duration::from_secs(5);

/// Minimum acceptable throughput ratio compared to single-path (80%)
#[allow(dead_code)]
const MIN_THROUGHPUT_RATIO: f64 = 0.8;

/// Path metrics tracked during testing
#[allow(dead_code)]
#[derive(Debug, Clone, Default)]
struct PathMetrics {
    bytes_sent: u64,
    bytes_received: u64,
    packets_sent: u64,
    packets_lost: u64,
    rtt_samples: Vec<Duration>,
    active: bool,
}

impl PathMetrics {
    #[allow(dead_code)]
    fn loss_rate(&self) -> f64 {
        if self.packets_sent == 0 {
            return 0.0;
        }
        self.packets_lost as f64 / self.packets_sent as f64
    }

    #[allow(dead_code)]
    fn avg_rtt(&self) -> Option<Duration> {
        if self.rtt_samples.is_empty() {
            return None;
        }
        let sum: Duration = self.rtt_samples.iter().sum();
        Some(sum / self.rtt_samples.len() as u32)
    }

    #[allow(dead_code)]
    fn throughput_mbps(&self, duration: Duration) -> f64 {
        if duration.as_secs_f64() == 0.0 {
            return 0.0;
        }
        let bits = (self.bytes_received * 8) as f64;
        let megabits = bits / 1_000_000.0;
        megabits / duration.as_secs_f64()
    }
}

/// Multipath transfer test context
#[allow(dead_code)]
struct MultipathTestContext {
    harness: TestHarness,
    path_metrics: Arc<RwLock<Vec<PathMetrics>>>,
}

impl MultipathTestContext {
    #[allow(dead_code)]
    async fn new(network_configs: Vec<NetworkConfig>) -> TestResult<Self> {
        let mut harness = TestHarness::new();
        let mut path_metrics = Vec::new();

        // Spawn one daemon per path to simulate multipath routing
        for (i, _config) in network_configs.iter().enumerate() {
            let daemon_id = format!("daemon_{}", i);
            let daemon_config = DaemonConfig {
                bind_addr: format!("127.0.0.1:{}", 8000 + i).parse().unwrap(),
                ..Default::default()
            };

            harness.spawn_daemon(&daemon_id, daemon_config).await?;
            path_metrics.push(PathMetrics::default());
        }

        Ok(Self {
            harness,
            path_metrics: Arc::new(RwLock::new(path_metrics)),
        })
    }

    #[allow(dead_code)]
    async fn connect_clients(&mut self) -> TestResult<Vec<ClientHandle>> {
        let mut clients = Vec::new();

        for i in 0..self.harness.daemon("daemon_0").is_some() as usize {
            let daemon_id = format!("daemon_{}", i);
            let client_id = format!("client_{}", i);

            self.harness.connect_client(&client_id, &daemon_id).await?;

            // SAFETY: We just connected the client, so it must exist
            let client = self
                .harness
                .client(&client_id)
                .expect("Client should exist after connection");

            // Clone the client handle for independent usage
            // Note: This is a simplified approach; production code would need
            // proper client handle cloning or shared reference management
            clients.push(ClientHandle::connect(&client_id, client.daemon_addr()).await?);
        }

        Ok(clients)
    }

    #[allow(dead_code)]
    async fn shutdown(mut self) -> TestResult<()> {
        self.harness.shutdown_all().await
    }
}

/// Test concurrent data transfer over two paths
#[tokio::test]
#[ignore] // Requires running daemon
async fn test_dual_path_concurrent_transfer() -> TestResult<()> {
    // Initialize tracing for test debugging
    let _ = tracing_subscriber::fmt().with_test_writer().try_init();

    info!("Starting dual-path concurrent transfer test");

    // Setup two paths with ideal network conditions
    let network_configs = vec![
        NetworkConfig::default(), // Primary path: ideal
        NetworkConfig::default(), // Secondary path: ideal
    ];

    let mut ctx = MultipathTestContext::new(network_configs).await?;

    // Generate test payload
    let payload: Vec<u8> = (0..TEST_PAYLOAD_SIZE).map(|i| (i % 256) as u8).collect();

    info!("Test payload size: {} bytes", payload.len());

    // Connect clients to both paths
    let clients = ctx.connect_clients().await?;
    assert_eq!(clients.len(), 2, "Should have two client connections");

    // Measure single-path throughput (baseline)
    info!("Measuring single-path baseline throughput");
    let single_path_start = Instant::now();
    clients[0].send(&payload).await?;
    let mut recv_buf = vec![0u8; TEST_PAYLOAD_SIZE];
    let n = clients[0].recv(&mut recv_buf).await?;
    let single_path_duration = single_path_start.elapsed();

    assert_eq!(n, TEST_PAYLOAD_SIZE, "Should receive full payload");
    assert_eq!(&recv_buf[..n], &payload[..], "Payload should match");

    let single_path_throughput = {
        let bits = (n * 8) as f64;
        let megabits = bits / 1_000_000.0;
        megabits / single_path_duration.as_secs_f64()
    };

    info!(
        "Single-path throughput: {:.2} Mbps ({:?})",
        single_path_throughput, single_path_duration
    );

    // Measure dual-path throughput (split payload across both paths)
    info!("Measuring dual-path throughput");
    let dual_path_start = Instant::now();

    let mid = payload.len() / 2;
    let (chunk1, chunk2) = payload.split_at(mid);

    // Send both chunks concurrently
    let send_task1 = clients[0].send(chunk1);
    let send_task2 = clients[1].send(chunk2);

    tokio::try_join!(send_task1, send_task2)?;

    // Receive both chunks concurrently
    let mut recv_buf1 = vec![0u8; mid];
    let mut recv_buf2 = vec![0u8; payload.len() - mid];

    let recv_task1 = clients[0].recv(&mut recv_buf1);
    let recv_task2 = clients[1].recv(&mut recv_buf2);

    let (n1, n2) = tokio::try_join!(recv_task1, recv_task2)?;
    let dual_path_duration = dual_path_start.elapsed();

    assert_eq!(n1, mid, "Should receive first half");
    assert_eq!(n2, payload.len() - mid, "Should receive second half");
    assert_eq!(&recv_buf1[..n1], chunk1, "First chunk should match");
    assert_eq!(&recv_buf2[..n2], chunk2, "Second chunk should match");

    let dual_path_throughput = {
        let bits = ((n1 + n2) * 8) as f64;
        let megabits = bits / 1_000_000.0;
        megabits / dual_path_duration.as_secs_f64()
    };

    info!(
        "Dual-path throughput: {:.2} Mbps ({:?})",
        dual_path_throughput, dual_path_duration
    );

    // Verify throughput improvement or at least maintained performance
    let throughput_ratio = dual_path_throughput / single_path_throughput;
    info!("Throughput ratio (dual/single): {:.2}", throughput_ratio);

    assert!(
        throughput_ratio >= MIN_THROUGHPUT_RATIO,
        "Dual-path throughput should be at least {}% of single-path (actual: {:.1}%)",
        MIN_THROUGHPUT_RATIO * 100.0,
        throughput_ratio * 100.0
    );

    // Cleanup
    ctx.shutdown().await?;
    info!("Dual-path concurrent transfer test completed successfully");
    Ok(())
}

/// Test failover behavior when primary path degrades
#[tokio::test]
#[ignore] // Requires running daemon
async fn test_path_failover() -> TestResult<()> {
    let _ = tracing_subscriber::fmt().with_test_writer().try_init();

    info!("Starting path failover test");

    // Setup two paths: primary (ideal) and fallback (ideal initially)
    let network_configs = vec![
        NetworkConfig::default(), // Primary path
        NetworkConfig::default(), // Fallback path
    ];

    let mut ctx = MultipathTestContext::new(network_configs).await?;
    let clients = ctx.connect_clients().await?;

    // Generate test payload
    let payload: Vec<u8> = (0..1024).map(|i| (i % 256) as u8).collect();

    // Phase 1: Normal operation on primary path
    info!("Phase 1: Sending data via primary path");
    clients[0].send(&payload).await?;
    let mut recv_buf = vec![0u8; payload.len()];
    let n = clients[0].recv(&mut recv_buf).await?;
    assert_eq!(n, payload.len(), "Should receive full payload on primary");
    assert_eq!(&recv_buf[..n], &payload[..], "Payload should match");

    // Phase 2: Simulate primary path degradation (in production, this would be
    // detected by PathBuilder via probe metrics, but here we manually trigger)
    info!("Phase 2: Simulating primary path degradation");
    warn!("Primary path degraded - initiating failover");

    // Attempt to use primary (should fail or timeout in production)
    // For this test, we'll directly use the fallback path
    let failover_start = Instant::now();

    info!("Phase 3: Sending data via fallback path");
    clients[1].send(&payload).await?;
    let mut recv_buf2 = vec![0u8; payload.len()];
    let n2 = clients[1].recv(&mut recv_buf2).await?;
    let failover_duration = failover_start.elapsed();

    assert_eq!(n2, payload.len(), "Should receive full payload on fallback");
    assert_eq!(&recv_buf2[..n2], &payload[..], "Payload should match");

    info!("Failover completed in {:?}", failover_duration);

    // Verify failover time is within acceptable threshold
    assert!(
        failover_duration <= FAILOVER_THRESHOLD,
        "Failover should complete within {:?} (actual: {:?})",
        FAILOVER_THRESHOLD,
        failover_duration
    );

    // Cleanup
    ctx.shutdown().await?;
    info!("Path failover test completed successfully");
    Ok(())
}

/// Test path quality metrics collection
#[tokio::test]
#[ignore] // Requires running daemon
async fn test_path_quality_metrics() -> TestResult<()> {
    let _ = tracing_subscriber::fmt().with_test_writer().try_init();

    info!("Starting path quality metrics test");

    // Setup paths with varying network conditions
    let network_configs = vec![
        NetworkConfig {
            latency_ms: 10,
            jitter_ms: 2,
            loss_rate: 0.01, // 1% loss
            bandwidth_bps: None,
        },
        NetworkConfig {
            latency_ms: 50,
            jitter_ms: 10,
            loss_rate: 0.05, // 5% loss
            bandwidth_bps: None,
        },
    ];

    let mut ctx = MultipathTestContext::new(network_configs).await?;
    let _clients = ctx.connect_clients().await?;

    // Simulate path probing and metric collection
    // In production, this would be handled by NetworkPathProber
    info!("Simulating path probing");

    let mut metrics = ctx.path_metrics.write().await;

    // Simulate metrics for path 0 (good quality)
    metrics[0].packets_sent = 100;
    metrics[0].packets_lost = 1;
    metrics[0].bytes_sent = 10240;
    metrics[0].bytes_received = 10240;
    metrics[0].rtt_samples = vec![Duration::from_millis(10); 10];
    metrics[0].active = true;

    // Simulate metrics for path 1 (degraded quality)
    metrics[1].packets_sent = 100;
    metrics[1].packets_lost = 5;
    metrics[1].bytes_sent = 10240;
    metrics[1].bytes_received = 9728; // Some data lost
    metrics[1].rtt_samples = vec![Duration::from_millis(50); 10];
    metrics[1].active = true;

    drop(metrics); // Release write lock

    // Verify path quality metrics
    {
        let metrics_read = ctx.path_metrics.read().await;

        let path0_loss = metrics_read[0].loss_rate();
        let path1_loss = metrics_read[1].loss_rate();

        info!("Path 0 loss rate: {:.2}%", path0_loss * 100.0);
        info!("Path 1 loss rate: {:.2}%", path1_loss * 100.0);

        assert!(path0_loss < 0.02, "Path 0 should have low loss rate");
        assert!(path1_loss >= 0.04, "Path 1 should have higher loss rate");

        let path0_rtt = metrics_read[0].avg_rtt().unwrap();
        let path1_rtt = metrics_read[1].avg_rtt().unwrap();

        info!("Path 0 avg RTT: {:?}", path0_rtt);
        info!("Path 1 avg RTT: {:?}", path1_rtt);

        assert!(
            path0_rtt < Duration::from_millis(20),
            "Path 0 should have low RTT"
        );
        assert!(
            path1_rtt >= Duration::from_millis(40),
            "Path 1 should have higher RTT"
        );
    } // Drop metrics_read here

    // Cleanup
    ctx.shutdown().await?;
    info!("Path quality metrics test completed successfully");
    Ok(())
}

/// Test multipath scheduling with weighted distribution
#[tokio::test]
#[ignore] // Requires running daemon and PathBuilder integration
async fn test_multipath_scheduling() -> TestResult<()> {
    let _ = tracing_subscriber::fmt().with_test_writer().try_init();

    info!("Starting multipath scheduling test");

    // Setup paths with different quality scores
    let network_configs = vec![
        NetworkConfig::default(), // High-quality path
        NetworkConfig {
            latency_ms: 100,
            jitter_ms: 20,
            loss_rate: 0.1, // Lower-quality path
            bandwidth_bps: None,
        },
    ];

    let mut ctx = MultipathTestContext::new(network_configs).await?;
    let clients = ctx.connect_clients().await?;

    // Send multiple packets and track distribution
    let num_packets = 100;
    let mut path0_count = 0;
    let mut path1_count = 0;

    info!("Sending {} packets with weighted scheduling", num_packets);

    for i in 0..num_packets {
        let payload = vec![i as u8; 64];

        // Simplified scheduling: prefer high-quality path (path 0)
        // In production, this would use PathBuilder's quality scores
        let use_path0 = (i % 10) < 7; // 70% on path 0, 30% on path 1

        if use_path0 {
            clients[0].send(&payload).await?;
            path0_count += 1;
        } else {
            clients[1].send(&payload).await?;
            path1_count += 1;
        }
    }

    info!(
        "Packet distribution: {} on path0, {} on path1",
        path0_count, path1_count
    );

    // Verify scheduling favors high-quality path
    assert!(
        path0_count > path1_count,
        "High-quality path should receive more packets"
    );

    let path0_ratio = path0_count as f64 / num_packets as f64;
    info!("Path 0 utilization: {:.1}%", path0_ratio * 100.0);

    assert!(
        path0_ratio >= 0.6,
        "High-quality path should handle at least 60% of traffic"
    );

    // Cleanup
    ctx.shutdown().await?;
    info!("Multipath scheduling test completed successfully");
    Ok(())
}
